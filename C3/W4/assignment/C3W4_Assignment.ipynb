{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bFWbEb6uGbN-"
   },
   "source": [
    "# Week 4: Predicting the next word\n",
    "\n",
    "Welcome to this assignment! During this week you saw how to create a model that will predict the next word in a text sequence, now you will implement such model and train it using a corpus of [Shakespeare Sonnets](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), while also creating some helper functions to pre-process the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "- You can add the comment # grade-up-to-here in any graded cell to signal the grader that it must only evaluate up to that point. This is helpful if you want to check if you are on the right track even if you are not done with the whole assignment. Be sure to remember to delete the comment afterwards!\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- To submit your notebook, save it and then click on the blue submit button at the beginning of the page.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BOwsuGQQY9OL",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Defining some useful global variables\n",
    "\n",
    "Next you will define some global variables that will be used throughout the assignment. Feel free to reference them in the upcoming exercises:\n",
    "\n",
    "- `FILE_PATH`: The file path where the sonnets file is located. \n",
    "\n",
    "- `NUM_BATCHES`: Number of batches. Defaults to 16.\n",
    "- `LSTM_UNITS`: Number of LSTM units in the LSTM layer.\n",
    "- `EMBEDDING_DIM`: Number of dimensions in the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "FILE_PATH = './data/sonnets.txt'\n",
    "NUM_BATCHES = 16\n",
    "LSTM_UNITS = 128\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A note about grading:**\n",
    "\n",
    "**When you submit this assignment for grading these same values for these globals will be used so make sure that all your code works well with these values. After submitting and passing this assignment, you are encouraged to come back here and play with these parameters to see the impact they have in the classification process. Since this next cell is frozen, you will need to copy the contents into a new cell and run it to overwrite the values for these globals.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Reading the dataset\n",
    "\n",
    "For this assignment you will be using the [Shakespeare Sonnets Dataset](https://www.opensourceshakespeare.org/views/sonnets/sonnet_view.php?range=viewrange&sonnetrange1=1&sonnetrange2=154), which contains more than 2000 lines of text extracted from Shakespeare's sonnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Pfd-nYKij5yY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2159 lines of sonnets\n",
      "\n",
      "The first 5 lines look like this:\n",
      "\n",
      "from fairest creatures we desire increase,\n",
      "that thereby beauty's rose might never die,\n",
      "but as the riper should by time decease,\n",
      "his tender heir might bear his memory:\n",
      "but thou, contracted to thine own bright eyes,\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "with open(FILE_PATH) as f:\n",
    "    data = f.read()\n",
    "\n",
    "# Convert to lower case and save as a list\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "print(f\"There are {len(corpus)} lines of sonnets\\n\")\n",
    "print(f\"The first 5 lines look like this:\\n\")\n",
    "for i in range(5):\n",
    "  print(corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "imB15zrSNhA1"
   },
   "source": [
    "## Exercise 1: fit_vectorizer\n",
    "\n",
    "In this exercise, you will use the [tf.keras.layers.TextVectorization layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) to tokenize and transform the text into numeric values. \n",
    "\n",
    "Note that in this case you will not pad the sentences right now as you've done before, because you need to build the n-grams before padding, so pay attention with the appropriate arguments passed to the TextVectorization layer!\n",
    "\n",
    "**Note**:\n",
    "- You should remove the punctuation and use only lowercase words, so you must pass the correct argument to TextVectorization layer.\n",
    "\n",
    "- In this case you will not pad the sentences with the TextVectorization layer as you've done before, because you need to build the n-grams before padding. Remember that by default, the TextVectorization layer will return a Tensor and therefore every element in it must have the same size, so if you pass two sentences of different length to be parsed, they will be padded. If you do not want to do that, you need to either pass the parameter ragged=True, or pass only a single sentence at the time. Later on in the assignment you will build the n-grams and depending on how you will iterate over the sentences, this may be important. If you choose to first pass the entire corpus to the TextVectorization and then perform the iteration, then you should pass ragged=True, otherwise, if you use the TextVectorization on each sentence separately, then you should not worry about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus (including the out of vocabulary): 3189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'and', 'the', 'to']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_vectorizer(corpus):\n",
    "    \"\"\"\n",
    "    Instantiates the vectorizer class on the corpus\n",
    "    \n",
    "    Args:\n",
    "        corpus (list): List with the sentences.\n",
    "    \n",
    "    Returns:\n",
    "        (tf.keras.layers.TextVectorization): an instance of the TextVectorization class containing the word-index dictionary, adapted to the corpus sentences.\n",
    "    \"\"\"    \n",
    "\n",
    "    tf.keras.utils.set_random_seed(65) # Do not change this line or you may have different expected outputs throughout the assignment\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define the object\n",
    "    vectorizer = tf.keras.layers.TextVectorization(ragged=True)\n",
    "    \n",
    "    # Adapt it to the corpus\n",
    "    vectorizer.adapt(corpus)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus (including the out of vocabulary): 3189\n"
     ]
    }
   ],
   "source": [
    "vectorizer = fit_vectorizer(corpus)\n",
    "total_words = len(vectorizer.get_vocabulary())\n",
    "print(f\"Total number of words in corpus (including the out of vocabulary): {total_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "```\n",
    "Total number of words in corpus (including the out of vocabulary): 3189\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77-0sA46OETa"
   },
   "source": [
    "One thing to note is that you can either pass a string or a list of strings to vectorizer. If you pass the former, it will return a *tensor* whereas if you pass the latter, it will return a *ragged tensor* if you've correctly configured the TextVectorization layer to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tqhPxdeXlfjh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passing a string directly: <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  29,   14,   18,    1, 1679])>\n",
      "Passing a list of strings: <tf.RaggedTensor [[29, 14, 18, 1, 1679]]>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Passing a string directly: {vectorizer('This is a test string').__repr__()}\")\n",
    "print(f\"Passing a list of strings: {vectorizer(['This is a test string'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "```\n",
    "Passing a string directly: <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  29,   14,   18,    1, 1679])>\n",
    "Passing a list of strings: <tf.RaggedTensor [[29, 14, 18, 1, 1679]]>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_fit_vectorizer(fit_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "-oqy9KjXRJ9A"
   },
   "source": [
    "## Generating n-grams\n",
    "\n",
    "As you saw in the lecture, the idea now is to generate the n-grams for each sentence in the corpus. So, for instance, if a vectorized sentence is given by `[45, 75, 195, 879]`, you must generate the following vectors:\n",
    "\n",
    "```Python\n",
    "[45, 75]\n",
    "[45, 75, 195]\n",
    "[45, 75, 195, 879]\n",
    "```\n",
    "## Exercise 2: n_grams_seqs\n",
    "\n",
    "Now complete the `n_gram_seqs` function below. This function receives the fitted vectorizer and the corpus (which is a list of strings) and should return a list containing the `n_gram` sequences for each line in the corpus.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "- If you pass `vectorizer(sentence)` the result is not padded, whereas if you pass `vectorizer(list_of_sentences)`, the result won't be padded **only if you passed the argument `ragged = True`** in the TextVectorization setup.\n",
    "- This exercise directly depends on the previous one, because you need to pass the defined vectorizer as a parameter, so any error thrown in the previous exercise may propagate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "id": "iy4baJMDl6kj",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: n_gram_seqs\n",
    "\n",
    "def n_gram_seqs(corpus, vectorizer):\n",
    "    \"\"\"\n",
    "    Generates a list of n-gram sequences\n",
    "    \n",
    "    Args:\n",
    "        corpus (list of string): lines of texts to generate n-grams for\n",
    "        vectorizer (tf.keras.layers.TextVectorization): an instance of the TextVectorization class adapted in the corpus\n",
    "    \n",
    "    Returns:\n",
    "        (list of tf.int64 tensors): the n-gram sequences for each line in the corpus\n",
    "    \"\"\"\n",
    "    input_sequences = []\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    corpus_sequence = vectorizer(corpus)\n",
    "    print(corpus_sequence)\n",
    "    for i in corpus_sequence:\n",
    "        for j in range(2, len(i)+1):\n",
    "            # input_sequences.append(vectorizer(i[:j]))\n",
    "            input_sequences.append(i[:j])\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return input_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DlKqW2pfM7G3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[35, 489, 1259, 164, 230, 582]]>\n",
      "n_gram sequences for first example look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 35, 489])>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([  35,  489, 1259])>,\n",
       " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  35,  489, 1259,  164])>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  35,  489, 1259,  164,  230])>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  35,  489, 1259,  164,  230,  582])>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with one example\n",
    "first_example_sequence = n_gram_seqs([corpus[0]], vectorizer)\n",
    "\n",
    "print(\"n_gram sequences for first example look like this:\\n\")\n",
    "first_example_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0HL8Ug6UU0Jt"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "n_gram sequences for first example look like this:\n",
    "\n",
    "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 35, 489])>,\n",
    " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([  35,  489, 1259])>,\n",
    " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  35,  489, 1259,  164])>,\n",
    " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  35,  489, 1259,  164,  230])>,\n",
    " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  35,  489, 1259,  164,  230,  582])>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wtPpCcBjNc4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[9, 935, 143, 369, 101, 171, 207], [17, 23, 3, 1006, 64, 31, 51, 803],\n",
      " [27, 315, 745, 101, 209, 27, 286]]>\n",
      "n_gram sequences for next 3 examples look like this:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([  9, 935])>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([  9, 935, 143])>,\n",
       " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  9, 935, 143, 369])>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  9, 935, 143, 369, 101])>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  9, 935, 143, 369, 101, 171])>,\n",
       " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([  9, 935, 143, 369, 101, 171, 207])>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([17, 23])>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([17, 23,  3])>,\n",
       " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  17,   23,    3, 1006])>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64])>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31])>,\n",
       " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31,   51])>,\n",
       " <tf.Tensor: shape=(8,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31,   51,  803])>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 27, 315])>,\n",
       " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([ 27, 315, 745])>,\n",
       " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 27, 315, 745, 101])>,\n",
       " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209])>,\n",
       " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209,  27])>,\n",
       " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209,  27, 286])>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with a bigger corpus\n",
    "next_3_examples_sequence = n_gram_seqs(corpus[1:4], vectorizer)\n",
    "\n",
    "print(\"n_gram sequences for next 3 examples look like this:\\n\")\n",
    "next_3_examples_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIzecMczU9UB"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "n_gram sequences for next 3 examples look like this:\n",
    "\n",
    "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([  9, 935])>,\n",
    " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([  9, 935, 143])>,\n",
    " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  9, 935, 143, 369])>,\n",
    " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  9, 935, 143, 369, 101])>,\n",
    " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  9, 935, 143, 369, 101, 171])>,\n",
    " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([  9, 935, 143, 369, 101, 171, 207])>,\n",
    " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([17, 23])>,\n",
    " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([17, 23,  3])>,\n",
    " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([  17,   23,    3, 1006])>,\n",
    " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64])>,\n",
    " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31])>,\n",
    " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31,   51])>,\n",
    " <tf.Tensor: shape=(8,), dtype=int64, numpy=array([  17,   23,    3, 1006,   64,   31,   51,  803])>,\n",
    " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([ 27, 315])>,\n",
    " <tf.Tensor: shape=(3,), dtype=int64, numpy=array([ 27, 315, 745])>,\n",
    " <tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 27, 315, 745, 101])>,\n",
    " <tf.Tensor: shape=(5,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209])>,\n",
    " <tf.Tensor: shape=(6,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209,  27])>,\n",
    " <tf.Tensor: shape=(7,), dtype=int64, numpy=array([ 27, 315, 745, 101, 209,  27, 286])>]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(7,), dtype=int64, numpy=array([589, 457, 163, 583, 190, 641, 467])>]\n",
      "[<tf.Tensor: shape=(2,), dtype=int64, numpy=array([783, 531])>, <tf.Tensor: shape=(7,), dtype=int64, numpy=array([  893,  1674, 29834,  2456,  1539, 23467, 90843])>]\n",
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_n_gram_seqs(n_gram_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dx3V_RjFWQSu"
   },
   "source": [
    "Apply the `n_gram_seqs` transformation to the whole corpus and save the maximum sequence length to use it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "laMwiRUpmuSd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[35, 489, 1259, 164, 230, 582], [9, 935, 143, 369, 101, 171, 207],\n",
      " [17, 23, 3, 1006, 64, 31, 51, 803], ...,\n",
      " [828, 138, 13, 493, 2, 29, 31, 9, 7, 188],\n",
      " [77, 268, 2438, 421, 421, 2885, 15, 20], []]>\n",
      "n_grams of input_sequences have length: 15355\n",
      "maximum length of sequences is: 11\n"
     ]
    }
   ],
   "source": [
    "# Apply the n_gram_seqs transformation to the whole corpus\n",
    "input_sequences = n_gram_seqs(corpus, vectorizer)\n",
    "\n",
    "# Save max length \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "print(f\"n_grams of input_sequences have length: {len(input_sequences)}\")\n",
    "print(f\"maximum length of sequences is: {max_sequence_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2OciMdmEdE9L"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "n_grams of input_sequences have length: 15355\n",
    "maximum length of sequences is: 11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zHY7HroqWq12"
   },
   "source": [
    "## Exercise 3: pad_seqs\n",
    "\n",
    "Now code the `pad_seqs` function which will pad any given sequences to the desired maximum length. Notice that this function receives a list of sequences and should return a numpy array with the padded sequences. You may have a look at the documentation of [`tf.keras.utils.pad_sequences`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences). \n",
    "\n",
    "**NOTE**: \n",
    "\n",
    "- Remember to pass the correct padding method as discussed in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "cellView": "code",
    "deletable": false,
    "id": "WW1-qAZaWOhC",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pad_seqs\n",
    "\n",
    "def pad_seqs(input_sequences, max_sequence_len):\n",
    "    \"\"\"\n",
    "    Pads tokenized sequences to the same length\n",
    "    \n",
    "    Args:\n",
    "        input_sequences (list of int): tokenized sequences to pad\n",
    "        maxlen (int): maximum length of the token sequences\n",
    "    \n",
    "    Returns:\n",
    "        (np.array of int32): tokenized sequences padded to the same length\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    padded_sequences = tf.keras.utils.pad_sequences(input_sequences, maxlen=max_sequence_len)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IqVQ0pb3YHLr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,   35,  489],\n",
       "       [   0,    0,    0,   35,  489, 1259],\n",
       "       [   0,    0,   35,  489, 1259,  164],\n",
       "       [   0,   35,  489, 1259,  164,  230],\n",
       "       [  35,  489, 1259,  164,  230,  582]], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the n_grams_seq of the first example\n",
    "first_padded_seq = pad_seqs(first_example_sequence, max([len(x) for x in first_example_sequence]))\n",
    "first_padded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Re_avDznXRnU"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "array([[   0,    0,    0,    0,   35,  489],\n",
    "       [   0,    0,    0,   35,  489, 1259],\n",
    "       [   0,    0,   35,  489, 1259,  164],\n",
    "       [   0,   35,  489, 1259,  164,  230],\n",
    "       [  35,  489, 1259,  164,  230,  582]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "j56_UCOBYzZt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    9,  935],\n",
       "       [   0,    0,    0,    0,    0,    9,  935,  143],\n",
       "       [   0,    0,    0,    0,    9,  935,  143,  369],\n",
       "       [   0,    0,    0,    9,  935,  143,  369,  101],\n",
       "       [   0,    0,    9,  935,  143,  369,  101,  171],\n",
       "       [   0,    9,  935,  143,  369,  101,  171,  207],\n",
       "       [   0,    0,    0,    0,    0,    0,   17,   23],\n",
       "       [   0,    0,    0,    0,    0,   17,   23,    3],\n",
       "       [   0,    0,    0,    0,   17,   23,    3, 1006],\n",
       "       [   0,    0,    0,   17,   23,    3, 1006,   64],\n",
       "       [   0,    0,   17,   23,    3, 1006,   64,   31],\n",
       "       [   0,   17,   23,    3, 1006,   64,   31,   51],\n",
       "       [  17,   23,    3, 1006,   64,   31,   51,  803],\n",
       "       [   0,    0,    0,    0,    0,    0,   27,  315],\n",
       "       [   0,    0,    0,    0,    0,   27,  315,  745],\n",
       "       [   0,    0,    0,    0,   27,  315,  745,  101],\n",
       "       [   0,    0,    0,   27,  315,  745,  101,  209],\n",
       "       [   0,    0,   27,  315,  745,  101,  209,   27],\n",
       "       [   0,   27,  315,  745,  101,  209,   27,  286]], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your function with the n_grams_seq of the next 3 examples\n",
    "next_3_padded_seq = pad_seqs(next_3_examples_sequence, max([len(s) for s in next_3_examples_sequence]))\n",
    "next_3_padded_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3rmcDluOXcIU"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "array([[   0,    0,    0,    0,    0,    0,    9,  935],\n",
    "       [   0,    0,    0,    0,    0,    9,  935,  143],\n",
    "       [   0,    0,    0,    0,    9,  935,  143,  369],\n",
    "       [   0,    0,    0,    9,  935,  143,  369,  101],\n",
    "       [   0,    0,    9,  935,  143,  369,  101,  171],\n",
    "       [   0,    9,  935,  143,  369,  101,  171,  207],\n",
    "       [   0,    0,    0,    0,    0,    0,   17,   23],\n",
    "       [   0,    0,    0,    0,    0,   17,   23,    3],\n",
    "       [   0,    0,    0,    0,   17,   23,    3, 1006],\n",
    "       [   0,    0,    0,   17,   23,    3, 1006,   64],\n",
    "       [   0,    0,   17,   23,    3, 1006,   64,   31],\n",
    "       [   0,   17,   23,    3, 1006,   64,   31,   51],\n",
    "       [  17,   23,    3, 1006,   64,   31,   51,  803],\n",
    "       [   0,    0,    0,    0,    0,    0,   27,  315],\n",
    "       [   0,    0,    0,    0,    0,   27,  315,  745],\n",
    "       [   0,    0,    0,    0,   27,  315,  745,  101],\n",
    "       [   0,    0,    0,   27,  315,  745,  101,  209],\n",
    "       [   0,    0,   27,  315,  745,  101,  209,   27],\n",
    "       [   0,   27,  315,  745,  101,  209,   27,  286]], dtype=int32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_pad_seqs(pad_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "rgK-Q_micEYA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padded corpus has shape: (15355, 11)\n"
     ]
    }
   ],
   "source": [
    "# Pad the whole corpus\n",
    "input_sequences = pad_seqs(input_sequences, max_sequence_len)\n",
    "\n",
    "print(f\"padded corpus has shape: {input_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59RD1YYNc7CW"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "padded corpus has shape: (15355, 11)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZbOidyPrXxf7"
   },
   "source": [
    "## Exercise 4: features_and_labels_dataset\n",
    "\n",
    "Before feeding the data into the neural network you should split it into features and labels. In this case the features will be the *padded n_gram sequences* with the **last element** removed from them and the labels will be the removed words.\n",
    "\n",
    "Complete the `features_and_labels_dataset` function below. This function expects the `padded n_gram sequences` as input and should return a **batched** [tensorflow dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) containing elements in the form (sentence, label). \n",
    "\n",
    "\n",
    "**NOTE**:\n",
    "- Notice that the function also receives the total of words in the corpus, this parameter will be **very important when one hot encoding the labels** since every word in the corpus will be a label at least once. The function you should use is [`tf.keras.utils.to_categorical`]((https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)).\n",
    "- To generate a dataset you may use the function [tf.data.Dataset.from_tensor_slices](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) after obtaining the sentences and their respective labels.\n",
    "- To batch a dataset, you may call the method [.batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch). A good number is `16`, but feel free to choose any number you want to, but keep it not greater than 64, otherwise the model may take too many epochs to achieve a good accuracy. Remember this is defined as a global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "code",
    "deletable": false,
    "id": "9WGGbYdnZdmJ",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: features_and_labels\n",
    "\n",
    "def features_and_labels_dataset(input_sequences, total_words):\n",
    "    \"\"\"\n",
    "    Generates features and labels from n-grams and returns a tensorflow dataset\n",
    "    \n",
    "    Args:\n",
    "        input_sequences (list of int): sequences to split features and labels from\n",
    "        total_words (int): vocabulary size\n",
    "    \n",
    "    Returns:\n",
    "        (tf.data.Dataset): Dataset with elements in the form (sentence, label)\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###\n",
    "    input_sequences = np.array(input_sequences)\n",
    "\n",
    "    # Define the features an labels as discussed in the lectures\n",
    "    features = input_sequences[:,:-1]\n",
    "    labels = input_sequences[:, -1]\n",
    "\n",
    "    # One hot encode the labels\n",
    "    one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n",
    "\n",
    "    # Build the dataset with the features and one hot encoded labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, one_hot_labels)) \n",
    "\n",
    "    # Batch de dataset with number of batches given by the global variable\n",
    "    batched_dataset = dataset.batch(NUM_BATCHES)\n",
    "\n",
    "    ### END CODE HERE ##\n",
    "\n",
    "    return batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "23DolaBRaIAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "\n",
      "N grams:\n",
      "\n",
      " [[   0    0    0    0   35]\n",
      " [   0    0    0   35  489]\n",
      " [   0    0   35  489 1259]\n",
      " [   0   35  489 1259  164]\n",
      " [  35  489 1259  164  230]]\n",
      "\n",
      "Label shape:\n",
      "\n",
      " (5, 3189)\n"
     ]
    }
   ],
   "source": [
    "# Test your function with the padded n_grams_seq of the first example\n",
    "dataset_example = features_and_labels_dataset(first_padded_seq, total_words)\n",
    "\n",
    "print(\"Example:\\n\")\n",
    "for features, label in dataset_example.take(1):\n",
    "    print(f\"N grams:\\n\\n {features}\\n\")\n",
    "    print(f\"Label shape:\\n\\n {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7t4yAx2UaQ43"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Example:\n",
    "\n",
    "N grams:\n",
    "\n",
    " [[   0    0    0    0   35]\n",
    " [   0    0    0   35  489]\n",
    " [   0    0   35  489 1259]\n",
    " [   0   35  489 1259  164]\n",
    " [  35  489 1259  164  230]]\n",
    "\n",
    "Label shape:\n",
    "\n",
    " (5, 3189)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_features_and_labels_dataset(features_and_labels_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Now let's generate the whole dataset that will be used for training. In this case, let's use the [.prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) method to speed up the training. Since the dataset is not that big, you should not have problems with memory by doing this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GRTuLEt3bRKa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: TensorSpec(shape=(None, 10), dtype=tf.int32, name=None)\n",
      "Label shape: TensorSpec(shape=(None, 3189), dtype=tf.float64, name=None)\n"
     ]
    }
   ],
   "source": [
    "# Split the whole corpus\n",
    "dataset = features_and_labels_dataset(input_sequences, total_words).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "print(f\"Feature shape: {dataset.element_spec[0]}\")\n",
    "print(f\"Label shape: {dataset.element_spec[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "xXSMK_HpdLns"
   },
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Feature shape: TensorSpec(shape=(None, 10), dtype=tf.int32, name=None)\n",
    "Label shape: TensorSpec(shape=(None, 3189), dtype=tf.float32, name=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ltxaOCE_aU6J"
   },
   "source": [
    "## Exercise 5: create_model\n",
    "\n",
    "Now you should define a model architecture capable of achieving an accuracy of at least 80%.\n",
    "\n",
    "Some hints to help you in this task:\n",
    "\n",
    "- The first layer in your model must be an [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) layer with the appropriate parameters, remember that your input are vectors with a fixed length size. Be careful with the size value you should pass as you've removed the last element of every input to be the label.\n",
    "\n",
    "- An appropriate `output_dim` for the first layer (Embedding) is 100, this is already provided for you.\n",
    "- A Bidirectional LSTM is helpful for this particular problem.\n",
    "- The last layer should have the same number of units as the total number of words in the corpus and a softmax activation function.\n",
    "- This problem can be solved with only two layers (excluding the Embedding and Input) so try out small architectures first.\n",
    "- 30 epochs should be enough to get an accuracy higher than 80%, if this is not the case try changing the architecture of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "cellView": "code",
    "deletable": false,
    "id": "XrE6kpJFfvRY",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_51\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_51\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">318,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_53                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3189</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,573</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3189</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,172,910</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_50 (\u001b[38;5;33mEmbedding\u001b[0m)        │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m100\u001b[0m)          │       \u001b[38;5;34m318,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_53                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m256\u001b[0m)              │       \u001b[38;5;34m234,496\u001b[0m │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3189\u001b[0m)             │       \u001b[38;5;34m819,573\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m3189\u001b[0m)             │    \u001b[38;5;34m10,172,910\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,545,879</span> (44.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,545,879\u001b[0m (44.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,545,879</span> (44.04 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,545,879\u001b[0m (44.04 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - accuracy: 0.0266 - loss: 6.9312\n",
      "Epoch 2/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0306 - loss: 5.8985\n",
      "Epoch 3/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0465 - loss: 5.4524\n",
      "Epoch 4/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0606 - loss: 5.0404\n",
      "Epoch 5/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0711 - loss: 4.7523\n",
      "Epoch 6/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0879 - loss: 4.4995\n",
      "Epoch 7/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.0981 - loss: 4.4423\n",
      "Epoch 8/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.1166 - loss: 4.1604\n",
      "Epoch 9/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.1474 - loss: 3.8367\n",
      "Epoch 10/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.1765 - loss: 3.6873\n",
      "Epoch 11/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.2094 - loss: 3.4424\n",
      "Epoch 12/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.2643 - loss: 3.0820\n",
      "Epoch 13/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.3398 - loss: 2.7160\n",
      "Epoch 14/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.3955 - loss: 2.4541\n",
      "Epoch 15/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4274 - loss: 2.2816\n",
      "Epoch 16/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.4917 - loss: 2.0397\n",
      "Epoch 17/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.5638 - loss: 1.6697\n",
      "Epoch 18/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.6280 - loss: 1.4043\n",
      "Epoch 19/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.6742 - loss: 1.2070\n",
      "Epoch 20/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7024 - loss: 1.1157\n",
      "Epoch 21/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7424 - loss: 0.9414\n",
      "Epoch 22/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.7718 - loss: 0.8143\n",
      "Epoch 23/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8045 - loss: 0.6882\n",
      "Epoch 24/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8147 - loss: 0.6400\n",
      "Epoch 25/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8353 - loss: 0.5704\n",
      "Epoch 26/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8449 - loss: 0.5302\n",
      "Epoch 27/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8584 - loss: 0.4837\n",
      "Epoch 28/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8572 - loss: 0.4719\n",
      "Epoch 29/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8682 - loss: 0.4456\n",
      "Epoch 30/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.8494 - loss: 0.5050\n"
     ]
    }
   ],
   "source": [
    "# GRADED FUNCTION: create_model\n",
    "\n",
    "def create_model(total_words, max_sequence_len):\n",
    "    \"\"\"\n",
    "    Creates a text generator model\n",
    "    \n",
    "    Args:\n",
    "        total_words (int): size of the vocabulary for the Embedding layer input\n",
    "        max_sequence_len (int): length of the input sequences\n",
    "    \n",
    "    Returns:\n",
    "       (tf.keras Model): the text generator model\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    model.add(tf.keras.layers.Input(shape=(max_sequence_len-1,), batch_size=NUM_BATCHES))\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=total_words,output_dim=EMBEDDING_DIM))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=LSTM_UNITS, return_sequences=False)))\n",
    "    model.add(tf.keras.layers.Dense(units = total_words, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(units = total_words, activation=\"softmax\"))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics = [\"accuracy\"])\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model\n",
    "\n",
    "# model = create_model(total_words, max_sequence_len)\n",
    "# model.summary()\n",
    "# example_batch = dataset.take(1)\n",
    "\n",
    "# try:\n",
    "# \tmodel.evaluate(example_batch, verbose=True)\n",
    "# except:\n",
    "# \tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "# else:\n",
    "# \tpredictions = model.predict(example_batch, verbose=False)\n",
    "# \tprint(f\"predictions have shape: {predictions.shape}\")\n",
    "# history = model.fit(dataset, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows you to check the number of total and trainable parameters of your model and prompts a warning in case these exceeds those of a reference solution, this serves the following 3 purposes listed in order of priority:\n",
    "\n",
    "- Helps you prevent crashing the kernel during training.\n",
    "\n",
    "- Helps you avoid longer-than-necessary training times.\n",
    "- Provides a reasonable estimate of the size of your model. In general you will usually prefer smaller models given that they accomplish their goal successfully.\n",
    "\n",
    "**Notice that this is just informative** and may be very well below the actual limit for size of the model necessary to crash the kernel. So even if you exceed this reference you are probably fine. However, **if the kernel crashes during training or it is taking a very long time and your model is larger than the reference, come back here and try to get the number of parameters closer to the reference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0IpX_Gu_gISk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour model has 852,189 total parameters and the reference is 2,000,000\u001b[92m. You are good to go!\n",
      "\n",
      "\u001b[92mYour model has 852,189 trainable parameters and the reference is 2,000,000\u001b[92m. You are good to go!\n"
     ]
    }
   ],
   "source": [
    "# Get the untrained model\n",
    "model = create_model(total_words, max_sequence_len)\n",
    "\n",
    "# Check the parameter count against a reference solution\n",
    "unittests.parameter_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions have shape: (16, 3189)\n"
     ]
    }
   ],
   "source": [
    "example_batch = dataset.take(1)\n",
    "\n",
    "try:\n",
    "\tmodel.evaluate(example_batch, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = model.predict(example_batch, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Expected output:**\n",
    "\n",
    "```\n",
    "predictions have shape: (NUM_BATCHES, 3189)\n",
    "```\n",
    "\n",
    "Where `NUM_BATCHES` is the number of batches you have set to your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your code!\n",
    "unittests.test_create_model(create_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.0248 - loss: 8.5248\n",
      "Epoch 2/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.0241 - loss: 6.7769\n",
      "Epoch 3/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.0243 - loss: 6.8592\n",
      "Epoch 4/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.0243 - loss: 6.8769\n",
      "Epoch 5/30\n",
      "\u001b[1m960/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.0243 - loss: 6.8797\n",
      "Epoch 6/30\n",
      "\u001b[1m757/960\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.0246 - loss: 6.8920"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(dataset, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gy72RPgly55q"
   },
   "source": [
    "**To pass this assignment, your model should achieve a training accuracy of at least 80%**. If your model didn't achieve this threshold, try training again with a different model architecture. Consider increasing the number of units in your `LSTM` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1fXTEO3GJ282",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get training and validation accuracies\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "# Get number of epochs\n",
    "epochs = range(len(acc))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "fig.suptitle('Training performance - Accuracy and Loss')\n",
    "\n",
    "for i, (data, label) in enumerate(zip([acc,loss], [\"Accuracy\", \"Loss\"])):\n",
    "    ax[i].plot(epochs, data, label=label)\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "OjvED5A3qrn2"
   },
   "source": [
    "If the accuracy meets the requirement of being greater than 80%, then save the `history.pkl` file which contains the information of the training history of your model and will be used to compute your grade. You can do this by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9QRG73l6qE-c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wdsMszk9zBs_"
   },
   "source": [
    "## See your model in action\n",
    "\n",
    "After all your work it is finally time to see your model generating text. \n",
    "\n",
    "Run the cell below to generate the next 100 words of a seed text.\n",
    "\n",
    "After submitting your assignment you are encouraged to try out training for different amounts of epochs and seeing how this affects the coherency of the generated text. Also try changing the seed text to see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "deletable": false,
    "id": "6Vc6PHgxa6Hm",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help me Obi Wan Kenobi, you're my only hope the truth vainly else prove thee more more perjured eye of bath desired lie and i my mistress brand newfired fell asleep weep which doth lie then still in my gross bodys treason ground this ill cruel but the ill same no ground for desired lie i i rise from fire took heat perpetual pride which found myself power not conscience hold it that i call love to my faults thy truth prove this holy fire of deeds desired conscience swearing i not conscience hold it i abhor my mistress foul took trial day desired true from my breast lies\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    # Convert the text into sequences\n",
    "    token_list = vectorizer(seed_text)\n",
    "    # Pad the sequences\n",
    "    token_list = tf.keras.utils.pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    # Get the probabilities of predicting a word\n",
    "    predicted = model.predict([token_list], verbose=0)\n",
    "    # Choose the next word based on the maximum probability\n",
    "    predicted = np.argmax(predicted, axis=-1).item()\n",
    "    # Get the actual word from the word index\n",
    "    output_word = vectorizer.get_vocabulary()[predicted]\n",
    "    # Append to the current text\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6r-X-HXtSc8N"
   },
   "source": [
    "**Congratulations on finishing this week's assignment!**\n",
    "\n",
    "You have successfully implemented a neural network capable of predicting the next word in a sequence of text!\n",
    "\n",
    "**We hope to see you in the next course of the specialization! Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "dlai_version": "1.2.0",
  "grader_version": "1",
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
